{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc98b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tree_sitter\n",
      "  Downloading tree_sitter-0.25.2-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting tree_sitter_javascript\n",
      "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting tree_sitter_typescript\n",
      "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting tree_sitter_python\n",
      "  Downloading tree_sitter_python-0.25.0-cp310-abi3-win_amd64.whl.metadata (1.9 kB)\n",
      "Downloading tree_sitter-0.25.2-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading tree_sitter_javascript-0.25.0-cp310-abi3-win_amd64.whl (62 kB)\n",
      "Downloading tree_sitter_typescript-0.23.2-cp39-abi3-win_amd64.whl (278 kB)\n",
      "Downloading tree_sitter_python-0.25.0-cp310-abi3-win_amd64.whl (76 kB)\n",
      "Installing collected packages: tree_sitter_typescript, tree_sitter_python, tree_sitter_javascript, tree_sitter\n",
      "Successfully installed tree_sitter-0.25.2 tree_sitter_javascript-0.25.0 tree_sitter_python-0.25.0 tree_sitter_typescript-0.23.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tree_sitter tree_sitter_javascript tree_sitter_typescript tree_sitter_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b891e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ REPOSITORY ANALYZER - Tree-sitter + AST\n",
      "======================================================================\n",
      "\n",
      "This tool analyzes LOCAL repositories and generates:\n",
      "  üìä Comprehensive statistics\n",
      "  üèóÔ∏è Dependency graphs (Mermaid diagrams)\n",
      "  üìù Full documentation\n",
      "  ‚öõÔ∏è Component analysis\n",
      "  üîó Call graphs\n",
      "======================================================================\n",
      "üìÇ Analyzing repository: E:\\AnwitaChakraborty\\Projects\\Campaign-AI\n",
      "  Parsed 30 files...\n",
      "‚úÖ Parsed 31 files\n",
      "üìä Generating dependency graph...\n",
      "üîó Generating call graph...\n",
      "üìù Generating documentation...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ANALYSIS COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìä Total files analyzed: 31\n",
      "üì¶ External dependencies: 12\n",
      "üîó Internal dependencies: 2\n",
      "‚ö° Functions found: 122\n",
      "\n",
      "üìÅ Results saved to: output/\n",
      "   üìÑ JSON: analysis.json\n",
      "   üìö Documentation: docs/\n",
      "\n",
      "üìñ Documentation files generated:\n",
      "   ‚Ä¢ README.md - Documentation index\n",
      "   ‚Ä¢ SUMMARY.md - Overview & statistics\n",
      "   ‚Ä¢ ARCHITECTURE.md - Dependency graphs\n",
      "   ‚Ä¢ FILES.md - Detailed file docs\n",
      "   ‚Ä¢ COMPONENTS.md - React/JSX components\n",
      "   ‚Ä¢ DEPENDENCIES.md - Dependency analysis\n",
      "   ‚Ä¢ ROUTES.md - Routes & pages\n",
      "   ‚Ä¢ CALL_GRAPH.md - Function listings\n",
      "\n",
      "======================================================================\n",
      "üìä Top 5 External Dependencies:\n",
      "======================================================================\n",
      "  ‚Ä¢ react: used in 9 files\n",
      "  ‚Ä¢ next: used in 5 files\n",
      "  ‚Ä¢ lucide-react: used in 5 files\n",
      "  ‚Ä¢ firebase: used in 3 files\n",
      "  ‚Ä¢ @: used in 2 files\n",
      "\n",
      "‚ú® Open output/docs/README.md to start exploring!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_javascript\n",
    "import tree_sitter_typescript\n",
    "import tree_sitter_python\n",
    "from collections import defaultdict\n",
    "\n",
    "SKIP_DIRECTORIES = {\n",
    "    'node_modules', '.git', '__pycache__', 'dist', 'build', '.next',\n",
    "    'coverage', '.pytest_cache', 'venv', 'env', '.venv', 'out'\n",
    "}\n",
    "\n",
    "LANGUAGE_EXTENSIONS = {\n",
    "    '.js': 'javascript', '.mjs': 'javascript', '.jsx': 'jsx',\n",
    "    '.ts': 'typescript', '.tsx': 'tsx', '.py': 'python'\n",
    "}\n",
    "\n",
    "def should_skip_directory(dirname):\n",
    "    return dirname in SKIP_DIRECTORIES or dirname.startswith('.')\n",
    "\n",
    "def get_file_language(filename):\n",
    "    for ext, lang in LANGUAGE_EXTENSIONS.items():\n",
    "        if filename.endswith(ext):\n",
    "            return lang\n",
    "    return None\n",
    "\n",
    "class JavaScriptParser:\n",
    "    def __init__(self):\n",
    "        self.language = Language(tree_sitter_javascript.language())\n",
    "        self.parser = Parser(self.language)\n",
    "    \n",
    "    def parse(self, file_path, rel_path, is_jsx=False):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        tree = self.parser.parse(bytes(content, 'utf8'))\n",
    "        root = tree.root_node\n",
    "        \n",
    "        result = {\n",
    "            'file': rel_path,\n",
    "            'language': 'jsx' if is_jsx else 'javascript',\n",
    "            'functions': [],\n",
    "            'classes': [],\n",
    "            'imports': [],\n",
    "            'exports': [],\n",
    "            'jsx': [],\n",
    "            'dependencies': [],\n",
    "            'variables': [],\n",
    "            'doc': ''\n",
    "        }\n",
    "        \n",
    "        self._traverse(root, result, content)\n",
    "        result['doc'] = self._generate_summary(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _traverse(self, node, result, content):\n",
    "        if node.type == 'import_statement':\n",
    "            self._extract_import(node, result, content)\n",
    "        elif node.type == 'function_declaration':\n",
    "            self._extract_function(node, result, content)\n",
    "        elif node.type == 'arrow_function' or node.type == 'function':\n",
    "            self._extract_arrow_function(node, result, content)\n",
    "        elif node.type == 'class_declaration':\n",
    "            self._extract_class(node, result, content)\n",
    "        elif node.type == 'export_statement':\n",
    "            self._extract_export(node, result, content)\n",
    "        elif node.type == 'jsx_element' or node.type == 'jsx_self_closing_element':\n",
    "            self._extract_jsx_element(node, result, content)\n",
    "        elif node.type == 'variable_declaration':\n",
    "            self._extract_variable(node, result, content)\n",
    "        \n",
    "        for child in node.children:\n",
    "            self._traverse(child, result, content)\n",
    "    \n",
    "    def _extract_import(self, node, result, content):\n",
    "        import_data = {'source': '', 'specifiers': [], 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'string':\n",
    "                source = content[child.start_byte:child.end_byte].strip('\"\\'')\n",
    "                import_data['source'] = source\n",
    "                if not source.startswith('.') and not source.startswith('/'):\n",
    "                    result['dependencies'].append(source.split('/')[0])\n",
    "        result['imports'].append(import_data)\n",
    "    \n",
    "    def _extract_function(self, node, result, content):\n",
    "        func_data = {'name': '', 'params': [], 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'identifier':\n",
    "                func_data['name'] = content[child.start_byte:child.end_byte]\n",
    "            elif child.type == 'formal_parameters':\n",
    "                func_data['params'] = self._extract_params(child, content)\n",
    "        result['functions'].append(func_data)\n",
    "    \n",
    "    def _extract_arrow_function(self, node, result, content):\n",
    "        parent = node.parent\n",
    "        func_data = {'name': 'anonymous', 'params': [], 'line': node.start_point[0] + 1}\n",
    "        \n",
    "        if parent and parent.type == 'variable_declarator':\n",
    "            for child in parent.children:\n",
    "                if child.type == 'identifier':\n",
    "                    func_data['name'] = content[child.start_byte:child.end_byte]\n",
    "        \n",
    "        for child in node.children:\n",
    "            if child.type == 'formal_parameters':\n",
    "                func_data['params'] = self._extract_params(child, content)\n",
    "        \n",
    "        if func_data['name'] != 'anonymous':\n",
    "            result['functions'].append(func_data)\n",
    "    \n",
    "    def _extract_class(self, node, result, content):\n",
    "        class_data = {'name': '', 'methods': [], 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'identifier':\n",
    "                class_data['name'] = content[child.start_byte:child.end_byte]\n",
    "            elif child.type == 'class_body':\n",
    "                self._extract_methods(child, class_data, content)\n",
    "        result['classes'].append(class_data)\n",
    "    \n",
    "    def _extract_methods(self, node, class_data, content):\n",
    "        for child in node.children:\n",
    "            if child.type == 'method_definition':\n",
    "                method_data = {'name': '', 'params': []}\n",
    "                for method_child in child.children:\n",
    "                    if method_child.type == 'property_identifier':\n",
    "                        method_data['name'] = content[method_child.start_byte:method_child.end_byte]\n",
    "                    elif method_child.type == 'formal_parameters':\n",
    "                        method_data['params'] = self._extract_params(method_child, content)\n",
    "                if method_data['name']:\n",
    "                    class_data['methods'].append(method_data)\n",
    "    \n",
    "    def _extract_params(self, node, content):\n",
    "        params = []\n",
    "        for child in node.children:\n",
    "            if child.type == 'identifier':\n",
    "                params.append(content[child.start_byte:child.end_byte])\n",
    "            elif child.type == 'required_parameter' or child.type == 'optional_parameter':\n",
    "                for param_child in child.children:\n",
    "                    if param_child.type == 'identifier':\n",
    "                        params.append(content[param_child.start_byte:param_child.end_byte])\n",
    "        return params\n",
    "    \n",
    "    def _extract_export(self, node, result, content):\n",
    "        export_data = {'type': 'unknown', 'name': '', 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'function_declaration' or child.type == 'class_declaration':\n",
    "                for subchild in child.children:\n",
    "                    if subchild.type == 'identifier':\n",
    "                        export_data['name'] = content[subchild.start_byte:subchild.end_byte]\n",
    "                        export_data['type'] = child.type.replace('_declaration', '')\n",
    "        result['exports'].append(export_data)\n",
    "    \n",
    "    def _extract_variable(self, node, result, content):\n",
    "        for child in node.children:\n",
    "            if child.type == 'variable_declarator':\n",
    "                var_name = ''\n",
    "                for var_child in child.children:\n",
    "                    if var_child.type == 'identifier':\n",
    "                        var_name = content[var_child.start_byte:var_child.end_byte]\n",
    "                if var_name:\n",
    "                    result['variables'].append({'name': var_name, 'line': node.start_point[0] + 1})\n",
    "    \n",
    "    def _extract_jsx_element(self, node, result, content):\n",
    "        jsx_data = {'type': '', 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'jsx_opening_element' or child.type == 'jsx_self_closing_element':\n",
    "                for elem_child in child.children:\n",
    "                    if elem_child.type == 'identifier':\n",
    "                        jsx_data['type'] = content[elem_child.start_byte:elem_child.end_byte]\n",
    "        if jsx_data['type']:\n",
    "            result['jsx'].append(jsx_data)\n",
    "    \n",
    "    def _generate_summary(self, result):\n",
    "        parts = []\n",
    "        if result['functions']:\n",
    "            parts.append(f\"{len(result['functions'])} functions\")\n",
    "        if result['classes']:\n",
    "            parts.append(f\"{len(result['classes'])} classes\")\n",
    "        if result['jsx']:\n",
    "            parts.append(f\"{len(result['jsx'])} JSX elements\")\n",
    "        if result['imports']:\n",
    "            parts.append(f\"{len(result['imports'])} imports\")\n",
    "        return f\"JavaScript/JSX file with {', '.join(parts) if parts else 'no exports'}\"\n",
    "\n",
    "class TypeScriptParser:\n",
    "    def __init__(self):\n",
    "        self.language = Language(tree_sitter_typescript.language_typescript())\n",
    "        self.parser = Parser(self.language)\n",
    "        self.tsx_language = Language(tree_sitter_typescript.language_tsx())\n",
    "        self.tsx_parser = Parser(self.tsx_language)\n",
    "    \n",
    "    def parse(self, file_path, rel_path, is_tsx=False):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        parser = self.tsx_parser if is_tsx else self.parser\n",
    "        tree = parser.parse(bytes(content, 'utf8'))\n",
    "        root = tree.root_node\n",
    "        \n",
    "        result = {\n",
    "            'file': rel_path,\n",
    "            'language': 'tsx' if is_tsx else 'typescript',\n",
    "            'functions': [],\n",
    "            'classes': [],\n",
    "            'imports': [],\n",
    "            'exports': [],\n",
    "            'jsx': [],\n",
    "            'dependencies': [],\n",
    "            'variables': [],\n",
    "            'doc': ''\n",
    "        }\n",
    "        \n",
    "        self._traverse(root, result, content)\n",
    "        result['doc'] = self._generate_summary(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _traverse(self, node, result, content):\n",
    "        if node.type == 'import_statement':\n",
    "            self._extract_import(node, result, content)\n",
    "        elif node.type == 'function_declaration':\n",
    "            self._extract_function(node, result, content)\n",
    "        elif node.type == 'arrow_function':\n",
    "            self._extract_arrow_function(node, result, content)\n",
    "        elif node.type == 'class_declaration':\n",
    "            self._extract_class(node, result, content)\n",
    "        elif node.type == 'export_statement':\n",
    "            self._extract_export(node, result, content)\n",
    "        elif node.type == 'jsx_element' or node.type == 'jsx_self_closing_element':\n",
    "            self._extract_jsx_element(node, result, content)\n",
    "        elif node.type == 'variable_declaration':\n",
    "            self._extract_variable(node, result, content)\n",
    "        \n",
    "        for child in node.children:\n",
    "            self._traverse(child, result, content)\n",
    "    \n",
    "    def _extract_import(self, node, result, content):\n",
    "        import_data = {'source': '', 'specifiers': [], 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'string':\n",
    "                source = content[child.start_byte:child.end_byte].strip('\"\\'')\n",
    "                import_data['source'] = source\n",
    "                if not source.startswith('.') and not source.startswith('/'):\n",
    "                    result['dependencies'].append(source.split('/')[0])\n",
    "        result['imports'].append(import_data)\n",
    "    \n",
    "    def _extract_function(self, node, result, content):\n",
    "        func_data = {'name': '', 'params': [], 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'identifier':\n",
    "                func_data['name'] = content[child.start_byte:child.end_byte]\n",
    "            elif child.type == 'formal_parameters':\n",
    "                func_data['params'] = self._extract_params(child, content)\n",
    "        result['functions'].append(func_data)\n",
    "    \n",
    "    def _extract_arrow_function(self, node, result, content):\n",
    "        parent = node.parent\n",
    "        func_data = {'name': 'anonymous', 'params': [], 'line': node.start_point[0] + 1}\n",
    "        \n",
    "        if parent and parent.type == 'variable_declarator':\n",
    "            for child in parent.children:\n",
    "                if child.type == 'identifier':\n",
    "                    func_data['name'] = content[child.start_byte:child.end_byte]\n",
    "        \n",
    "        for child in node.children:\n",
    "            if child.type == 'formal_parameters':\n",
    "                func_data['params'] = self._extract_params(child, content)\n",
    "        \n",
    "        if func_data['name'] != 'anonymous':\n",
    "            result['functions'].append(func_data)\n",
    "    \n",
    "    def _extract_class(self, node, result, content):\n",
    "        class_data = {'name': '', 'methods': [], 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'type_identifier' or child.type == 'identifier':\n",
    "                class_data['name'] = content[child.start_byte:child.end_byte]\n",
    "            elif child.type == 'class_body':\n",
    "                self._extract_methods(child, class_data, content)\n",
    "        result['classes'].append(class_data)\n",
    "    \n",
    "    def _extract_methods(self, node, class_data, content):\n",
    "        for child in node.children:\n",
    "            if child.type == 'method_definition':\n",
    "                method_data = {'name': '', 'params': []}\n",
    "                for method_child in child.children:\n",
    "                    if method_child.type == 'property_identifier':\n",
    "                        method_data['name'] = content[method_child.start_byte:method_child.end_byte]\n",
    "                    elif method_child.type == 'formal_parameters':\n",
    "                        method_data['params'] = self._extract_params(method_child, content)\n",
    "                if method_data['name']:\n",
    "                    class_data['methods'].append(method_data)\n",
    "    \n",
    "    def _extract_params(self, node, content):\n",
    "        params = []\n",
    "        for child in node.children:\n",
    "            if child.type == 'identifier':\n",
    "                params.append(content[child.start_byte:child.end_byte])\n",
    "            elif child.type == 'required_parameter' or child.type == 'optional_parameter':\n",
    "                for param_child in child.children:\n",
    "                    if param_child.type == 'identifier':\n",
    "                        params.append(content[param_child.start_byte:param_child.end_byte])\n",
    "        return params\n",
    "    \n",
    "    def _extract_export(self, node, result, content):\n",
    "        export_data = {'type': 'unknown', 'name': '', 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'function_declaration' or child.type == 'class_declaration':\n",
    "                for subchild in child.children:\n",
    "                    if subchild.type == 'identifier' or subchild.type == 'type_identifier':\n",
    "                        export_data['name'] = content[subchild.start_byte:subchild.end_byte]\n",
    "                        export_data['type'] = child.type.replace('_declaration', '')\n",
    "        result['exports'].append(export_data)\n",
    "    \n",
    "    def _extract_variable(self, node, result, content):\n",
    "        for child in node.children:\n",
    "            if child.type == 'variable_declarator':\n",
    "                var_name = ''\n",
    "                for var_child in child.children:\n",
    "                    if var_child.type == 'identifier':\n",
    "                        var_name = content[var_child.start_byte:var_child.end_byte]\n",
    "                if var_name:\n",
    "                    result['variables'].append({'name': var_name, 'line': node.start_point[0] + 1})\n",
    "    \n",
    "    def _extract_jsx_element(self, node, result, content):\n",
    "        jsx_data = {'type': '', 'line': node.start_point[0] + 1}\n",
    "        for child in node.children:\n",
    "            if child.type == 'jsx_opening_element' or child.type == 'jsx_self_closing_element':\n",
    "                for elem_child in child.children:\n",
    "                    if elem_child.type == 'identifier':\n",
    "                        jsx_data['type'] = content[elem_child.start_byte:elem_child.end_byte]\n",
    "        if jsx_data['type']:\n",
    "            result['jsx'].append(jsx_data)\n",
    "    \n",
    "    def _generate_summary(self, result):\n",
    "        parts = []\n",
    "        if result['functions']:\n",
    "            parts.append(f\"{len(result['functions'])} functions\")\n",
    "        if result['classes']:\n",
    "            parts.append(f\"{len(result['classes'])} classes\")\n",
    "        if result['jsx']:\n",
    "            parts.append(f\"{len(result['jsx'])} JSX elements\")\n",
    "        if result['imports']:\n",
    "            parts.append(f\"{len(result['imports'])} imports\")\n",
    "        return f\"TypeScript/TSX file with {', '.join(parts) if parts else 'no exports'}\"\n",
    "\n",
    "class PythonParser:\n",
    "    def parse(self, file_path, rel_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        result = {\n",
    "            'file': rel_path,\n",
    "            'language': 'python',\n",
    "            'functions': [],\n",
    "            'classes': [],\n",
    "            'imports': [],\n",
    "            'exports': [],\n",
    "            'jsx': [],\n",
    "            'dependencies': [],\n",
    "            'variables': [],\n",
    "            'doc': ''\n",
    "        }\n",
    "        \n",
    "        self._traverse(tree, result)\n",
    "        result['doc'] = self._generate_summary(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _traverse(self, node, result):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for alias in node.names:\n",
    "                result['imports'].append({'source': alias.name, 'specifiers': [], 'line': node.lineno})\n",
    "                result['dependencies'].append(alias.name.split('.')[0])\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            if node.module:\n",
    "                result['imports'].append({'source': node.module, 'specifiers': [a.name for a in node.names], 'line': node.lineno})\n",
    "                if not node.module.startswith('.'):\n",
    "                    result['dependencies'].append(node.module.split('.')[0])\n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            func_data = {\n",
    "                'name': node.name,\n",
    "                'params': [arg.arg for arg in node.args.args],\n",
    "                'line': node.lineno,\n",
    "                'docstring': ast.get_docstring(node)\n",
    "            }\n",
    "            result['functions'].append(func_data)\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            class_data = {\n",
    "                'name': node.name,\n",
    "                'methods': [],\n",
    "                'line': node.lineno,\n",
    "                'docstring': ast.get_docstring(node)\n",
    "            }\n",
    "            for item in node.body:\n",
    "                if isinstance(item, ast.FunctionDef):\n",
    "                    method_data = {\n",
    "                        'name': item.name,\n",
    "                        'params': [arg.arg for arg in item.args.args]\n",
    "                    }\n",
    "                    class_data['methods'].append(method_data)\n",
    "            result['classes'].append(class_data)\n",
    "        elif isinstance(node, ast.Assign):\n",
    "            for target in node.targets:\n",
    "                if isinstance(target, ast.Name):\n",
    "                    result['variables'].append({'name': target.id, 'line': node.lineno})\n",
    "        \n",
    "        for child in ast.iter_child_nodes(node):\n",
    "            self._traverse(child, result)\n",
    "    \n",
    "    def _generate_summary(self, result):\n",
    "        parts = []\n",
    "        if result['functions']:\n",
    "            parts.append(f\"{len(result['functions'])} functions\")\n",
    "        if result['classes']:\n",
    "            parts.append(f\"{len(result['classes'])} classes\")\n",
    "        if result['imports']:\n",
    "            parts.append(f\"{len(result['imports'])} imports\")\n",
    "        return f\"Python file with {', '.join(parts) if parts else 'no exports'}\"\n",
    "\n",
    "class DependencyGraph:\n",
    "    def __init__(self, files_data):\n",
    "        self.files_data = files_data\n",
    "    \n",
    "    def generate(self):\n",
    "        graph = {\n",
    "            'nodes': [],\n",
    "            'edges': [],\n",
    "            'external_deps': {},\n",
    "            'file_dependencies': defaultdict(list),\n",
    "            'dependency_count': {}\n",
    "        }\n",
    "        \n",
    "        for file_data in self.files_data:\n",
    "            graph['nodes'].append(file_data['file'])\n",
    "            graph['dependency_count'][file_data['file']] = len(file_data['dependencies'])\n",
    "            \n",
    "            for imp in file_data['imports']:\n",
    "                source = imp['source']\n",
    "                if source.startswith('.'):\n",
    "                    resolved = self._resolve_relative(file_data['file'], source)\n",
    "                    if resolved:\n",
    "                        graph['edges'].append({\n",
    "                            'from': file_data['file'],\n",
    "                            'to': resolved,\n",
    "                            'line': imp.get('line', 0)\n",
    "                        })\n",
    "                        graph['file_dependencies'][file_data['file']].append(resolved)\n",
    "                else:\n",
    "                    pkg = source.split('/')[0]\n",
    "                    if pkg not in graph['external_deps']:\n",
    "                        graph['external_deps'][pkg] = []\n",
    "                    graph['external_deps'][pkg].append(file_data['file'])\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def _resolve_relative(self, current_file, import_path):\n",
    "        current_dir = Path(current_file).parent\n",
    "        \n",
    "        for ext in ['.js', '.jsx', '.ts', '.tsx', '.py', '/index.js', '/index.ts', '/index.tsx']:\n",
    "            test_path = str(current_dir / import_path) + ext\n",
    "            for file_data in self.files_data:\n",
    "                if file_data['file'].replace('\\\\', '/') == test_path.replace('\\\\', '/'):\n",
    "                    return file_data['file']\n",
    "        return None\n",
    "\n",
    "class CallGraph:\n",
    "    def __init__(self, files_data):\n",
    "        self.files_data = files_data\n",
    "    \n",
    "    def generate(self):\n",
    "        graph = {\n",
    "            'nodes': [],\n",
    "            'edges': [],\n",
    "            'function_map': {}\n",
    "        }\n",
    "        \n",
    "        for file_data in self.files_data:\n",
    "            for func in file_data['functions']:\n",
    "                func_id = f\"{file_data['file']}:{func['name']}\"\n",
    "                graph['function_map'][func['name']] = func_id\n",
    "                graph['nodes'].append({\n",
    "                    'id': func_id,\n",
    "                    'name': func['name'],\n",
    "                    'file': file_data['file'],\n",
    "                    'params': func['params'],\n",
    "                    'line': func['line']\n",
    "                })\n",
    "        \n",
    "        return graph\n",
    "\n",
    "class DocumentationGenerator:\n",
    "    def __init__(self, files_data, dependency_data, call_data, output_dir):\n",
    "        self.files_data = files_data\n",
    "        self.dependency_data = dependency_data\n",
    "        self.call_data = call_data\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.docs_dir = self.output_dir / 'docs'\n",
    "        self.docs_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def generate_all(self):\n",
    "        self._generate_summary()\n",
    "        self._generate_architecture()\n",
    "        self._generate_files()\n",
    "        self._generate_components()\n",
    "        self._generate_dependencies()\n",
    "        self._generate_routes()\n",
    "        self._generate_call_graph()\n",
    "        self._generate_index()\n",
    "    \n",
    "    def _generate_summary(self):\n",
    "        content = \"# Repository Summary\\n\\n\"\n",
    "        content += f\"**Total Files Analyzed:** {len(self.files_data)}\\n\\n\"\n",
    "        \n",
    "        by_lang = {}\n",
    "        for file_data in self.files_data:\n",
    "            lang = file_data['language']\n",
    "            by_lang[lang] = by_lang.get(lang, 0) + 1\n",
    "        \n",
    "        content += \"## Files by Language\\n\\n\"\n",
    "        content += \"| Language | Count |\\n|----------|-------|\\n\"\n",
    "        for lang, count in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "            content += f\"| {lang} | {count} |\\n\"\n",
    "        \n",
    "        total_funcs = sum(len(f['functions']) for f in self.files_data)\n",
    "        total_classes = sum(len(f['classes']) for f in self.files_data)\n",
    "        total_imports = sum(len(f['imports']) for f in self.files_data)\n",
    "        \n",
    "        content += f\"\\n## Code Statistics\\n\\n\"\n",
    "        content += f\"- **Total Functions:** {total_funcs}\\n\"\n",
    "        content += f\"- **Total Classes:** {total_classes}\\n\"\n",
    "        content += f\"- **Total Imports:** {total_imports}\\n\"\n",
    "        content += f\"- **External Dependencies:** {len(self.dependency_data['external_deps'])}\\n\"\n",
    "        content += f\"- **Internal Dependencies:** {len(self.dependency_data['edges'])}\\n\"\n",
    "        \n",
    "        content += \"\\n## Top External Dependencies\\n\\n\"\n",
    "        sorted_deps = sorted(self.dependency_data['external_deps'].items(), \n",
    "                            key=lambda x: len(x[1]), reverse=True)[:10]\n",
    "        for dep, files in sorted_deps:\n",
    "            content += f\"- **{dep}** (used in {len(files)} files)\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'SUMMARY.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def _generate_architecture(self):\n",
    "        content = \"# Architecture Overview\\n\\n\"\n",
    "        \n",
    "        content += \"## Project Structure\\n\\n\"\n",
    "        dirs = set()\n",
    "        for file_data in self.files_data:\n",
    "            parts = Path(file_data['file']).parts\n",
    "            for i in range(1, len(parts)):\n",
    "                dirs.add('/'.join(parts[:i]))\n",
    "        \n",
    "        for directory in sorted(dirs)[:20]:\n",
    "            files_count = sum(1 for f in self.files_data if f['file'].startswith(directory))\n",
    "            content += f\"- `{directory}/` ({files_count} files)\\n\"\n",
    "        \n",
    "        content += \"\\n## Dependency Graph (Top 30 Internal Dependencies)\\n\\n\"\n",
    "        content += \"```mermaid\\ngraph TD\\n\"\n",
    "        \n",
    "        for edge in self.dependency_data['edges'][:30]:\n",
    "            from_node = edge['from'].replace('/', '_').replace('.', '_').replace('-', '_')\n",
    "            to_node = edge['to'].replace('/', '_').replace('.', '_').replace('-', '_')\n",
    "            content += f\"    {from_node}[{Path(edge['from']).name}] --> {to_node}[{Path(edge['to']).name}]\\n\"\n",
    "        \n",
    "        content += \"```\\n\\n\"\n",
    "        \n",
    "        content += \"## External Dependencies Graph\\n\\n\"\n",
    "        content += \"```mermaid\\ngraph LR\\n\"\n",
    "        \n",
    "        for dep, files in list(self.dependency_data['external_deps'].items())[:15]:\n",
    "            dep_node = dep.replace('-', '_').replace('.', '_').replace('@', '')\n",
    "            content += f\"    {dep_node}[{dep}]\\n\"\n",
    "            for file in files[:3]:\n",
    "                file_node = Path(file).stem.replace('-', '_').replace('.', '_')\n",
    "                content += f\"    {file_node}[{Path(file).name}] --> {dep_node}\\n\"\n",
    "        \n",
    "        content += \"```\\n\\n\"\n",
    "        \n",
    "        content += \"## Most Connected Files\\n\\n\"\n",
    "        sorted_files = sorted(self.dependency_data['dependency_count'].items(), \n",
    "                             key=lambda x: x[1], reverse=True)[:10]\n",
    "        content += \"| File | Dependencies |\\n|------|-------------|\\n\"\n",
    "        for file, count in sorted_files:\n",
    "            content += f\"| {file} | {count} |\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'ARCHITECTURE.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def _generate_files(self):\n",
    "        content = \"# Files Documentation\\n\\n\"\n",
    "        content += f\"Total files documented: {len(self.files_data)}\\n\\n\"\n",
    "        \n",
    "        for file_data in sorted(self.files_data, key=lambda x: x['file']):\n",
    "            content += f\"## {file_data['file']}\\n\\n\"\n",
    "            content += f\"**Language:** {file_data['language']} | \"\n",
    "            content += f\"**Functions:** {len(file_data['functions'])} | \"\n",
    "            content += f\"**Classes:** {len(file_data['classes'])} | \"\n",
    "            content += f\"**Imports:** {len(file_data['imports'])}\\n\\n\"\n",
    "            \n",
    "            content += f\"*{file_data['doc']}*\\n\\n\"\n",
    "            \n",
    "            if file_data['imports']:\n",
    "                content += \"### Imports\\n\\n\"\n",
    "                for imp in file_data['imports'][:10]:\n",
    "                    content += f\"- `{imp['source']}`\"\n",
    "                    if 'line' in imp:\n",
    "                        content += f\" (line {imp['line']})\"\n",
    "                    content += \"\\n\"\n",
    "                content += \"\\n\"\n",
    "            \n",
    "            if file_data['functions']:\n",
    "                content += \"### Functions\\n\\n\"\n",
    "                for func in file_data['functions']:\n",
    "                    params = ', '.join(func['params']) if func['params'] else ''\n",
    "                    content += f\"#### `{func['name']}({params})`\\n\\n\"\n",
    "                    content += f\"- **Line:** {func['line']}\\n\"\n",
    "                    if func.get('docstring'):\n",
    "                        content += f\"- **Documentation:** {func['docstring'][:100]}...\\n\"\n",
    "                    content += \"\\n\"\n",
    "            \n",
    "            if file_data['classes']:\n",
    "                content += \"### Classes\\n\\n\"\n",
    "                for cls in file_data['classes']:\n",
    "                    content += f\"#### `class {cls['name']}`\\n\\n\"\n",
    "                    content += f\"- **Line:** {cls['line']}\\n\"\n",
    "                    if cls.get('docstring'):\n",
    "                        content += f\"- **Documentation:** {cls['docstring'][:100]}...\\n\"\n",
    "                    if cls['methods']:\n",
    "                        content += \"- **Methods:**\\n\"\n",
    "                        for method in cls['methods']:\n",
    "                            params = ', '.join(method['params']) if method['params'] else ''\n",
    "                            content += f\"  - `{method['name']}({params})`\\n\"\n",
    "                    content += \"\\n\"\n",
    "            \n",
    "            content += \"---\\n\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'FILES.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def _generate_components(self):\n",
    "        content = \"# Component Documentation\\n\\n\"\n",
    "        \n",
    "        react_files = [f for f in self.files_data if f['language'] in ['jsx', 'tsx']]\n",
    "        \n",
    "        if react_files:\n",
    "            content += f\"Total React/TSX files: {len(react_files)}\\n\\n\"\n",
    "            \n",
    "            content += \"## Component Hierarchy\\n\\n\"\n",
    "            content += \"```mermaid\\ngraph TD\\n\"\n",
    "            \n",
    "            for file_data in react_files[:30]:\n",
    "                if file_data['jsx']:\n",
    "                    file_node = Path(file_data['file']).stem.replace('-', '_').replace('.', '_')\n",
    "                    content += f\"    {file_node}[{Path(file_data['file']).name}]\\n\"\n",
    "                    jsx_types = set(j['type'] for j in file_data['jsx'])\n",
    "                    for jsx_type in list(jsx_types)[:5]:\n",
    "                        jsx_node = jsx_type.replace('-', '_')\n",
    "                        content += f\"    {file_node} --> {jsx_node}[{jsx_type}]\\n\"\n",
    "            \n",
    "            content += \"```\\n\\n\"\n",
    "            \n",
    "            content += \"## React Components\\n\\n\"\n",
    "            for file_data in react_files:\n",
    "                if file_data['jsx'] or file_data['functions']:\n",
    "                    content += f\"### {file_data['file']}\\n\\n\"\n",
    "                    \n",
    "                    if file_data['functions']:\n",
    "                        component_funcs = [f for f in file_data['functions'] if f['name'][0].isupper()]\n",
    "                        if component_funcs:\n",
    "                            content += \"**Components:**\\n\"\n",
    "                            for func in component_funcs:\n",
    "                                content += f\"- `{func['name']}` (line {func['line']})\\n\"\n",
    "                            content += \"\\n\"\n",
    "                    \n",
    "                    if file_data['jsx']:\n",
    "                        jsx_types = {}\n",
    "                        for j in file_data['jsx']:\n",
    "                            jsx_types[j['type']] = jsx_types.get(j['type'], 0) + 1\n",
    "                        content += \"**JSX Elements Used:**\\n\"\n",
    "                        for jsx_type, count in sorted(jsx_types.items()):\n",
    "                            content += f\"- `<{jsx_type}>` ({count} times)\\n\"\n",
    "                        content += \"\\n\"\n",
    "        else:\n",
    "            content += \"No React/JSX components found.\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'COMPONENTS.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def _generate_dependencies(self):\n",
    "        content = \"# Dependencies Analysis\\n\\n\"\n",
    "        \n",
    "        content += \"## External Dependencies\\n\\n\"\n",
    "        content += f\"Total external packages: {len(self.dependency_data['external_deps'])}\\n\\n\"\n",
    "        \n",
    "        sorted_deps = sorted(self.dependency_data['external_deps'].items(), \n",
    "                            key=lambda x: len(x[1]), reverse=True)\n",
    "        \n",
    "        content += \"| Package | Files Using | Files |\\n|---------|-------------|-------|\\n\"\n",
    "        for dep, files in sorted_deps[:30]:\n",
    "            file_list = ', '.join([Path(f).name for f in files[:3]])\n",
    "            if len(files) > 3:\n",
    "                file_list += f\" +{len(files)-3} more\"\n",
    "            content += f\"| `{dep}` | {len(files)} | {file_list} |\\n\"\n",
    "        \n",
    "        content += \"\\n## Internal File Dependencies\\n\\n\"\n",
    "        \n",
    "        if self.dependency_data['file_dependencies']:\n",
    "            content += \"```mermaid\\ngraph TD\\n\"\n",
    "            \n",
    "            for file, deps in list(self.dependency_data['file_dependencies'].items())[:25]:\n",
    "                file_node = Path(file).stem.replace('-', '_').replace('.', '_')\n",
    "                for dep in deps[:3]:\n",
    "                    dep_node = Path(dep).stem.replace('-', '_').replace('.', '_')\n",
    "                    content += f\"    {file_node}[{Path(file).name}] --> {dep_node}[{Path(dep).name}]\\n\"\n",
    "            \n",
    "            content += \"```\\n\\n\"\n",
    "        \n",
    "        content += \"## Dependency Tree (Most Connected)\\n\\n\"\n",
    "        for file, deps in sorted(self.dependency_data['file_dependencies'].items(), \n",
    "                                 key=lambda x: len(x[1]), reverse=True)[:15]:\n",
    "            content += f\"### {file}\\n\\n\"\n",
    "            content += f\"Depends on {len(deps)} files:\\n\"\n",
    "            for dep in deps[:10]:\n",
    "                content += f\"- {dep}\\n\"\n",
    "            content += \"\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'DEPENDENCIES.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def _generate_routes(self):\n",
    "        content = \"# Routes & Pages\\n\\n\"\n",
    "        \n",
    "        route_patterns = ['app/', 'pages/', 'routes/', 'views/']\n",
    "        route_files = [f for f in self.files_data \n",
    "                      if any(pattern in f['file'] for pattern in route_patterns)]\n",
    "        \n",
    "        if route_files:\n",
    "            content += f\"Total route files detected: {len(route_files)}\\n\\n\"\n",
    "            \n",
    "            content += \"## Route Structure\\n\\n\"\n",
    "            content += \"```mermaid\\ngraph TD\\n\"\n",
    "            \n",
    "            for file_data in route_files[:30]:\n",
    "                file_parts = Path(file_data['file']).parts\n",
    "                if len(file_parts) > 1:\n",
    "                    parent_node = '_'.join(file_parts[:-1]).replace('-', '_')\n",
    "                    file_node = file_parts[-1].replace('-', '_').replace('.', '_')\n",
    "                    content += f\"    {parent_node} --> {file_node}[{file_parts[-1]}]\\n\"\n",
    "            \n",
    "            content += \"```\\n\\n\"\n",
    "            \n",
    "            content += \"## Route Files\\n\\n\"\n",
    "            for file_data in sorted(route_files, key=lambda x: x['file']):\n",
    "                content += f\"### {file_data['file']}\\n\\n\"\n",
    "                \n",
    "                if file_data['functions']:\n",
    "                    content += \"**Handlers/Components:**\\n\"\n",
    "                    for func in file_data['functions']:\n",
    "                        content += f\"- `{func['name']}` (line {func['line']})\\n\"\n",
    "                    content += \"\\n\"\n",
    "                \n",
    "                if file_data['exports']:\n",
    "                    content += \"**Exports:**\\n\"\n",
    "                    for exp in file_data['exports']:\n",
    "                        if exp['name']:\n",
    "                            content += f\"- {exp['type']}: `{exp['name']}`\\n\"\n",
    "                    content += \"\\n\"\n",
    "        else:\n",
    "            content += \"No route files detected.\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'ROUTES.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def _generate_call_graph(self):\n",
    "        content = \"# Call Graph & Functions\\n\\n\"\n",
    "        \n",
    "        content += f\"Total functions: {len(self.call_data['nodes'])}\\n\\n\"\n",
    "        \n",
    "        content += \"## Function Distribution by File\\n\\n\"\n",
    "        func_by_file = defaultdict(list)\n",
    "        for node in self.call_data['nodes']:\n",
    "            func_by_file[node['file']].append(node)\n",
    "        \n",
    "        sorted_files = sorted(func_by_file.items(), \n",
    "                             key=lambda x: len(x[1]), reverse=True)[:20]\n",
    "        \n",
    "        content += \"| File | Functions |\\n|------|----------|\\n\"\n",
    "        for file, funcs in sorted_files:\n",
    "            func_names = ', '.join([f['name'] for f in funcs[:5]])\n",
    "            if len(funcs) > 5:\n",
    "                func_names += f\" +{len(funcs)-5} more\"\n",
    "            content += f\"| {file} | {func_names} |\\n\"\n",
    "        \n",
    "        content += \"\\n## All Functions\\n\\n\"\n",
    "        \n",
    "        for file, funcs in sorted(func_by_file.items()):\n",
    "            content += f\"### {file}\\n\\n\"\n",
    "            for func in sorted(funcs, key=lambda x: x['line']):\n",
    "                params = ', '.join(func['params']) if func['params'] else ''\n",
    "                content += f\"- `{func['name']}({params})` - line {func['line']}\\n\"\n",
    "            content += \"\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'CALL_GRAPH.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def _generate_index(self):\n",
    "        content = \"# Repository Documentation Index\\n\\n\"\n",
    "        content += \"Welcome to the auto-generated documentation.\\n\\n\"\n",
    "        content += \"## Documentation Files\\n\\n\"\n",
    "        content += \"- [üìä Summary](SUMMARY.md) - Overview and statistics\\n\"\n",
    "        content += \"- [üèóÔ∏è Architecture](ARCHITECTURE.md) - Dependency graphs and structure\\n\"\n",
    "        content += \"- [üìÅ Files](FILES.md) - Detailed file documentation\\n\"\n",
    "        content += \"- [‚öõÔ∏è Components](COMPONENTS.md) - React/JSX components\\n\"\n",
    "        content += \"- [üì¶ Dependencies](DEPENDENCIES.md) - External and internal dependencies\\n\"\n",
    "        content += \"- [üõ£Ô∏è Routes](ROUTES.md) - Application routes and pages\\n\"\n",
    "        content += \"- [üîó Call Graph](CALL_GRAPH.md) - Function listings and relationships\\n\"\n",
    "        \n",
    "        with open(self.docs_dir / 'README.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "class RepositoryAnalyzer:\n",
    "    def __init__(self, repo_path, output_dir):\n",
    "        self.repo_path = Path(repo_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.js_parser = JavaScriptParser()\n",
    "        self.ts_parser = TypeScriptParser()\n",
    "        self.py_parser = PythonParser()\n",
    "        self.files_data = []\n",
    "    \n",
    "    def analyze(self):\n",
    "        print(f\"üìÇ Analyzing repository: {self.repo_path}\")\n",
    "        self._walk_repository()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Parsed {len(self.files_data)} files\")\n",
    "        print(\"üìä Generating dependency graph...\")\n",
    "        dep_graph = DependencyGraph(self.files_data)\n",
    "        dependency_data = dep_graph.generate()\n",
    "        \n",
    "        print(\"üîó Generating call graph...\")\n",
    "        call_graph = CallGraph(self.files_data)\n",
    "        call_data = call_graph.generate()\n",
    "        \n",
    "        print(\"üìù Generating documentation...\")\n",
    "        doc_gen = DocumentationGenerator(\n",
    "            self.files_data,\n",
    "            dependency_data,\n",
    "            call_data,\n",
    "            str(self.output_dir)\n",
    "        )\n",
    "        doc_gen.generate_all()\n",
    "        \n",
    "        return {\n",
    "            \"repository\": str(self.repo_path),\n",
    "            \"total_files\": len(self.files_data),\n",
    "            \"files\": self.files_data,\n",
    "            \"dependencies\": dependency_data,\n",
    "            \"call_graph\": call_data\n",
    "        }\n",
    "    \n",
    "    def _walk_repository(self):\n",
    "        for root, dirs, files in os.walk(self.repo_path):\n",
    "            dirs[:] = [d for d in dirs if not should_skip_directory(d)]\n",
    "            \n",
    "            for file in files:\n",
    "                file_path = Path(root) / file\n",
    "                language = get_file_language(file)\n",
    "                \n",
    "                if language:\n",
    "                    try:\n",
    "                        file_data = self._parse_file(file_path, language)\n",
    "                        if file_data:\n",
    "                            self.files_data.append(file_data)\n",
    "                            if len(self.files_data) % 10 == 0:\n",
    "                                print(f\"  Parsed {len(self.files_data)} files...\", end='\\r')\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è  Error parsing {file_path}: {e}\")\n",
    "    \n",
    "    def _parse_file(self, file_path, language):\n",
    "        rel_path = str(file_path.relative_to(self.repo_path)).replace('\\\\', '/')\n",
    "        \n",
    "        if language == 'javascript':\n",
    "            return self.js_parser.parse(str(file_path), rel_path)\n",
    "        elif language == 'jsx':\n",
    "            return self.js_parser.parse(str(file_path), rel_path, is_jsx=True)\n",
    "        elif language == 'typescript':\n",
    "            return self.ts_parser.parse(str(file_path), rel_path)\n",
    "        elif language == 'tsx':\n",
    "            return self.ts_parser.parse(str(file_path), rel_path, is_tsx=True)\n",
    "        elif language == 'python':\n",
    "            return self.py_parser.parse(str(file_path), rel_path)\n",
    "        \n",
    "        return None\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ REPOSITORY ANALYZER - Tree-sitter + AST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThis tool analyzes LOCAL repositories and generates:\")\n",
    "print(\"  üìä Comprehensive statistics\")\n",
    "print(\"  üèóÔ∏è Dependency graphs (Mermaid diagrams)\")\n",
    "print(\"  üìù Full documentation\")\n",
    "print(\"  ‚öõÔ∏è Component analysis\")\n",
    "print(\"  üîó Call graphs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "repo_path = input(\"\\nüìÇ Enter the LOCAL repository path: \").strip()\n",
    "\n",
    "if repo_path.startswith(('http://', 'https://', 'git@')):\n",
    "    print(\"\\n‚ùå ERROR: You provided a URL!\")\n",
    "    print(\"\\n1. Clone first: git clone\", repo_path)\n",
    "    print(\"2. Run again with local path\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.isdir(repo_path):\n",
    "    print(f\"\\n‚ùå ERROR: '{repo_path}' is not a directory!\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "analyzer = RepositoryAnalyzer(repo_path, str(output_dir))\n",
    "results = analyzer.analyze()\n",
    "\n",
    "json_output = output_dir / \"analysis.json\"\n",
    "with open(json_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Total files analyzed: {results['total_files']}\")\n",
    "print(f\"üì¶ External dependencies: {len(results['dependencies']['external_deps'])}\")\n",
    "print(f\"üîó Internal dependencies: {len(results['dependencies']['edges'])}\")\n",
    "print(f\"‚ö° Functions found: {len(results['call_graph']['nodes'])}\")\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {output_dir}/\")\n",
    "print(f\"   üìÑ JSON: analysis.json\")\n",
    "print(f\"   üìö Documentation: docs/\")\n",
    "print(f\"\\nüìñ Documentation files generated:\")\n",
    "print(f\"   ‚Ä¢ README.md - Documentation index\")\n",
    "print(f\"   ‚Ä¢ SUMMARY.md - Overview & statistics\") \n",
    "print(f\"   ‚Ä¢ ARCHITECTURE.md - Dependency graphs\")\n",
    "print(f\"   ‚Ä¢ FILES.md - Detailed file docs\")\n",
    "print(f\"   ‚Ä¢ COMPONENTS.md - React/JSX components\")\n",
    "print(f\"   ‚Ä¢ DEPENDENCIES.md - Dependency analysis\")\n",
    "print(f\"   ‚Ä¢ ROUTES.md - Routes & pages\")\n",
    "print(f\"   ‚Ä¢ CALL_GRAPH.md - Function listings\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä Top 5 External Dependencies:\")\n",
    "print(\"=\" * 70)\n",
    "sorted_deps = sorted(results['dependencies']['external_deps'].items(), \n",
    "                    key=lambda x: len(x[1]), reverse=True)[:5]\n",
    "for dep, files in sorted_deps:\n",
    "    print(f\"  ‚Ä¢ {dep}: used in {len(files)} files\")\n",
    "\n",
    "print(\"\\n‚ú® Open output/docs/README.md to start exploring!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71be85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
